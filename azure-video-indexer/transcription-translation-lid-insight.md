---
title: Get media transcription, translation, and language identification insights
description: This article shows you how to get Azure AI Video Indexer Media transcription, translation, and language identification insights.
author: bandersmsft
ms.author: banders
ms.collection: ce-skilling-ai-copilot
ms.date: 10/06/2025
ms.update-cycle: 180-days
ms.service: azure-video-indexer
ms.topic: how-to
appliesto:
  - Azure AI Video Indexer enabled by Azure Arc
  - Cloud-based Azure AI Video Indexer
---

# Get media transcription, translation, and language identification insights

The transcription, translation, and language identification features detect, transcribe, and translate speech in media files into more than 50 languages.

Azure AI Video Indexer (VI) processes the speech in the audio file to extract the transcription that it then translates into many languages. When you select a specific language for translation, both the transcription and the insights like keywords, topics, labels, or OCR are translated into the specified language. You can use the transcription as is or combine it with speaker insights that map and assign the transcripts to speakers. The audio file can contain multiple speakers. Each speaker receives an ID that's displayed under their transcribed speech.

**Language identification (LID)** recognizes the supported dominant spoken language in the video file. For more information, see [Applying LID](/azure/azure-video-indexer/language-identification-model). 

**Multi-language identification (MLID)** automatically recognizes the spoken languages in different segments in the audio file and sends each segment to be transcribed in the identified languages. At the end of this process, all transcriptions are combined into the same file. For more information, see [Applying MLID](/azure/azure-video-indexer/multi-language-identification-transcription).
The resulting insights are generated in a categorized list in a JSON file that includes the ID, language, transcribed text, duration, and confidence score.

When Azure AI Video Indexer indexes media files with multiple speakers, it performs speaker *diarization*. It identifies each speaker in a video and attributes each transcribed line to a speaker. The speakers receive unique identities such as Speaker #1 and Speaker #2. This feature allows for the identification of speakers during conversations and can be useful in various scenarios such as doctor-patient conversations, agent-customer interactions, and court proceedings. 

## Media transcription, translation, and language identification use cases 

- Promote accessibility by making content available for people with hearing disabilities. Use Azure AI Video Indexer to generate speech-to-text transcription and translation into multiple languages.
- Improve content distribution to a diverse audience in different regions and languages. Deliver content in multiple languages by using Azure AI Video Indexer’s transcription and translation capabilities. 
- Enhance and improve manual closed captioning and subtitles generation. Use Azure AI Video Indexer’s transcription and translation capabilities and the closed captions generated by Azure AI Video Indexer in one of the supported formats.
- Transcribe videos in unknown languages by using language identification (LID) or multilangauge identification (MLID). These features allow Azure AI Video Indexer to automatically identify the languages appearing in the video and generate the transcription accordingly.

## View the insight JSON with the web portal

After you upload and index a video, download insights in JSON format from the web portal.

1. Select the **Library** tab.
1. Select the media you want.
1. Select **Download**, then select **Insights (JSON)**. The JSON file opens in a new browser tab.
1. Find the key pair described in the example response.

## Use the API

1. Use a [Get Video Index](https://api-portal.videoindexer.ai/api-details#api=Operations&operation=Get-Video-Index) request. Pass `&includeSummarizedInsights=false`.
1. Find the key pairs described in the example response.

## Example response

The API returns all the languages it detects in the video under `sourceLanguage`. Each instance in the transcription section includes the transcribed language.

```json
    "insights": {
      "version": "1.0.0.0",
      "duration": "0:01:50.486",
      "sourceLanguage": "en-US",
      "sourceLanguages": [
        "es-ES",
        "en-US"
      ],
      "language": "en-US",
      "languages": [
        "en-US"
      ],
      "transcript": [
        {
          "id": 1,
          "text": "Hi, I'm Doug from office. We're talking about new features that office insiders will see first and I have a program manager,",
          "confidence": 0.8879,
          "speakerId": 1,
          "language": "en-US",
          "instances": [
            {
              "adjustedStart": "0:00:00",
              "adjustedEnd": "0:00:05.75",
              "start": "0:00:00",
              "end": "0:00:05.75"
            }
          ]
        },
        {
          "id": 2,
          "text": "Emily Tran, with office graphics.",
          "confidence": 0.8879,
          "speakerId": 1,
          "language": "en-US",
          "instances": [
            {
              "adjustedStart": "0:00:05.75",
              "adjustedEnd": "0:00:07.01",
              "start": "0:00:05.75",
              "end": "0:00:07.01"
            }
          ]
        },
```

> [!IMPORTANT]
> Read the [transparency note overview](/legal/azure-video-indexer/transparency-note?context=/azure/azure-video-indexer/context/context) for all Azure AI Video Indexer features. Each insight also has its own transparency note.

[!INCLUDE [transparency-transcription-translation-lid](./includes/transparency-transcription-translation-lid.md)]

## Sample code

[See all samples for Azure AI Video Indexer](https://github.com/Azure-Samples/azure-video-indexer-samples)

