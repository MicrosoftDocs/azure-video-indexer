---
author: inhenkel
ms.topic: include 
ms.service: azure-video-indexer
ms.collection: ce-skilling-ai-copilot,rai-skilling-ai-copilot
ms.date: 10/09/2024
ms.author: inhenkel
title: transparency key terms
---

## Key terms and features

| Term | Definition |
|--|--|
| Text-based emotion detection | Emotions such as joy, sadness, anger, and fear that were detected via transcript analysis. |
| Insight | The information and knowledge derived from the processing and analysis of video and audio files that generate different types of insights and can include detected objects, people, faces, key frames and translations or transcriptions. To view and download insights via the API, use the [Azure AI Video Indexer portal](https://api-portal.videoindexer.ai/). |
| Object detection | The ability to identify and find objects in an image or video. For example, a table, chair, or window. |
| Facial detection | Finds human faces in an image and returns bounding boxes indicating their locations. Face detection models alone do not find individually identifying features, only a bounding box marking the entire face. Facial detection doesn't involve distinguishing one fact from another face, predicting or classifying facial attributes, or creating a Face template. |
| Facial identification | "One-to-many" matching of a face in an unmanipulated image to a set of faces in a secure repository. An example is a touchless access control system in a building that replaces or augments physical cards and badges in which a smart camera captures the face of one person entering a secured door and attempts to find a match from a set of images of faces of individuals who are approved to access the building. This process is implemented by Azure AI Face service and involves the creation of Face templates. |
| Face template | Unique set of numbers generated from an image or video that represents the distinctive features of a face. |
| Observed people detection and matched faces | Features that automatically detect and match people in media files. Observed people detection and matched faces can be set to display insights on people, their clothing, and the exact time frame of their appearance. |
| Keyword extraction | The process of automatically detecting insights on the different keywords discussed in media files. Keywords extraction can extract insights in both single language and multi-language media files. |
| Deep search | The ability to retrieve only relevant video and audio files from a video library by searching for specific terms within the extracted insights. |
| Labels | The identification of visual objects and actions appearing in a frame. For example, identifying an object such as a dog, or an action such as running. |
| Named entities | Feature that uses Natural Language Processing (NLP) to extract insights on the locations, people and brands appearing in audio and images in media files. |
| Natural Language Processing (NLP) | The processing of human language as it is spoken and written. |
| Optical Character Recognition (OCR) | Extracts text from images like pictures, street signs, and products in media files to create insights. For more information, see [OCR technology](/azure/ai-services/computer-vision/overview-ocr). |
| Hierarchical Ontology Model | A set of concepts or categories in a subject area or domain that possess shared properties and relationships. |
| Audio effects detection | Feature that detects insights on a variety of acoustic events and classifies them into acoustic categories. Audio effect detection can detect and classify different categories such as laughter, crowd reactions, alarms and/or sirens. |
| Transcription, translation and language identification | Feature that automatically detects, transcribes, and translates the speech in media files into over 50 languages.  |
| Topics inference | Feature that automatically creates inferred insights derived from the transcribed audio, OCR content in visual text, and celebrities recognized in the video.  |
| Speaker diarization | Feature that identifies each speaker in a video and attributes each transcribed line to a speaker. This allows for the identification of speakers during conversations and can be useful in a variety of scenarios. |
| Bring Your Own Model | Feature that allows you to send insights and artifacts generated by Azure AI Video Indexer to external AI models. |
| Textual Video Summarization | Feature that summarizes the uses artificial intelligence to summarize the content of a video. |